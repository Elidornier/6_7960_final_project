<!DOCTYPE html>
<html>
<head>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
    /* 1. BASIC PAGE SETUP */
    body {
        background-color: #ffffff; /* Clean white background */
        color: #333333; /* Dark gray text is easier on eyes than pitch black */
        font-family: "Georgia", "Times New Roman", serif; /* Serif font for the main reading */
        font-size: 20px; /* Distill uses large, readable text */
        line-height: 1.6em; /* More breathing room between lines */
        margin: 0;
    }

    /* 2. LINKS (Blue) */
    a:link, a:visited {
        color: #2e6db2; 
        text-decoration: none;
        border-bottom: 1px solid #e0e0e0; /* Subtle underline */
    }
    a:hover {
        color: #1a4e85;
        border-bottom: 2px solid #1a4e85;
    }

    /* 3. LAYOUT GRID */
    .content-margin-container {
        display: flex;
        width: 100%;
        justify-content: center;
        align-items: flex-start; /* Aligns content to top */
        margin-bottom: 20px;
    }

    /* 4. MAIN COLUMN (Narrower for readability) */
    .main-content-block {
        width: 55%; 
        max-width: 720px; /* Optimal reading width */
        background-color: #fff;
        padding: 0px;
        border: none; 
    }

    /* 5. MARGIN NOTES */
    .margin-left-block {
        width: 20%;
        max-width: 180px;
        margin-right: 30px;
        text-align: right;
        font-size: 14px;
        color: #6b6b6b;
        font-family: "HelveticaNeue-Light", "Helvetica Neue", sans-serif;
        line-height: 1.4em;
    }

    .margin-right-block {
        width: 20%;
        max-width: 220px;
        margin-left: 30px;
        text-align: left;
        font-size: 13px;
        color: #6b6b6b;
        font-family: "HelveticaNeue-Light", "Helvetica Neue", sans-serif;
        line-height: 1.4em;
    }

    /* 6. HEADERS */
    h1, h2, h3 {
        font-family: "HelveticaNeue-Light", "Helvetica Neue", Helvetica, Arial, sans-serif;
        color: #000;
        margin-top: 50px;
        margin-bottom: 15px;
        font-weight: 600;
        line-height: 1.2em;
    }
    h1 { font-size: 36px; letter-spacing: -0.5px; }
    h2 { font-size: 26px; border-bottom: 1px solid #eee; padding-bottom: 10px; }
    h3 { font-size: 20px; font-weight: 600; margin-top: 30px;}

    /* 7. IMAGES & CAPTIONS */
    img, .my-video {
        max-width: 100%;
        height: auto;
        display: block;
        margin: 20px auto;
        border-radius: 4px; 
        box-shadow: 0 4px 10px rgba(0,0,0,0.05);
    }

    /* 8. MATH & CODE */
    .mathjax-mobile, .mathml-non-mobile { display: none; }
    .show-mathml .mathml-non-mobile { display: block; }
    .show-mathjax .mathjax-mobile { display: block; }
    
    code {
        background-color: #f4f4f4;
        padding: 2px 5px;
        border-radius: 3px;
        font-family: "Courier New", Courier, monospace;
        font-size: 0.9em;
    }

    /* 9. CITATION BLOCK */
    div.citation {
        font-family: "HelveticaNeue-Light", "Helvetica Neue", sans-serif;
        font-size: 14px;
        background-color: #f7f7f7;
        padding: 20px;
        border-radius: 5px;
        border: 1px solid #eee;
    }

    /* 10. TITLE HEADER TABLE SPECIFIC */
    table.header {
        width: 100%;
        max-width: 100%;
    }

    /* 11. TOP NAVIGATION BAR */
    .nav-header {
        background-color: #23374d; /* Navy Blue */
        width: 100%;
        display: flex;
        justify-content: center;
        padding: 15px 0;
        font-family: "HelveticaNeue-Light", "Helvetica Neue", Helvetica, Arial, sans-serif;
        border-bottom: 1px solid rgba(0,0,0,0.1);
    }
    
    .nav-inner {
        width: 90%; 
        max-width: 1080px; 
        display: flex;
        justify-content: space-between;
        align-items: center;
    }

    .nav-logo {
        font-weight: 600;
        font-size: 18px;
        color: white !important;
        border-bottom: none !important;
        display: flex;
        align-items: center;
    }

    .nav-links a {
        color: rgba(255,255,255,0.7) !important;
        margin-left: 25px;
        font-size: 12px;
        font-weight: 500;
        text-transform: uppercase;
        letter-spacing: 0.5px;
        border-bottom: none !important;
        transition: color 0.2s;
    }

    .nav-links a:hover {
        color: #ffffff !important;
    }

</style>

<title>Trajectory of Augmentation Difficulty</title>
<meta property="og:title" content="Trajectory of Augmentation Difficulty" />
<meta charset="UTF-8">
</head>

<body>

<div class="nav-header">
    <div class="nav-inner">
        <a href="#" class="nav-logo">
            <span class="logo-icon"></span> 6.7960 Project
        </a>
        
        <div class="nav-links">
            <a href="#">About</a>
            <a href="#">Paper</a>
            <a href="#">Code</a>
            <a href="#">Team</a>
        </div>
    </div>
</div>

<div class="content-margin-container" style="margin-top: 60px; margin-bottom: 60px;">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
        <h1 style="font-size: 50px; line-height: 1.1em; margin-bottom: 20px; font-family: 'Helvetica Neue', Helvetica, sans-serif;">
            Investigating the Trajectory<br>of Augmentation Difficulty
        </h1>
        
        <div style="font-family: 'Helvetica Neue', sans-serif; font-size: 18px; color: #555;">
            <p>
                <a href="#">Alice Xiao</a>, 
                <a href="#">Aimee Yu</a>, 
                <a href="#">Celinda Zhu</a>
            </p>
            <p style="font-size: 15px; color: #888;">
                Final Project for 6.7960, MIT &bull; Fall 2025
            </p>
        </div>
    </div>
    <div class="margin-right-block"></div>
</div>

<div class="content-margin-container" id="intro">
    <div class="margin-left-block">
        <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
            <b style="font-size:16px">Outline</b><br><br>
            <a href="#intro">Introduction</a><br><br>
            <a href="#related_work">Related Work</a><br><br>
            <a href="#methodology">Methodology</a><br><br>
            <a href="#experiments">Results</a><br><br>
            <a href="#discussion">Discussion</a><br><br>
        </div>
    </div>
    <div class="main-content-block">
        <h1>1. Introduction</h1>
        
        <h3>Motivation</h3>
        <p>
            Deep learning's remarkable success in computer vision can be attributed in large part to the capacity of powerful, over-parameterized models such as ResNet <a href="#ref_1">[1]</a> to generalize effectively beyond their training data. Consequently, Data Augmentation has emerged as a cornerstone technique to bridge the generalization gap, artificially expanding the training distribution through stochastic transformations like blurring, cropping, and occlusion.
        </p>
        <p>
            In our homework, we explored flipping and found it to be an interesting topic, deciding to dig deeper. However, standard data augmentation pipelines typically treat the training process as <strong>static</strong>. Augmentations are applied with random magnitudes drawn from a fixed distribution, regardless of the model's current learning state. This creates a fundamental misalignment between the data and the model:
        </p>
        <ul>
            <li>In <strong>early training stages</strong>, the model is trying to learn basic, low-level features like edges or shapes. Strong augmentations (such as massive cutout holes or high-frequency noise) might introduce excessive information loss, essentially "blinding" the model before it can see.</li>
            <li>In <strong>later training stages</strong>, the model has likely memorized the training data. Weak augmentations may fail to provide sufficient regularization, leading to overfitting.</li>
        </ul>
        <p>
            We hypothesize that the <strong>timing</strong> of difficulty is just as critical as the difficulty itself. Just as a human student would struggle if handed a complex calculus problem on day one, a neural network might struggle to converge if the signal-to-noise ratio is too low at initialization.
        </p>

        <h3>Curriculum Learning</h3>
        <p>
            Curriculum Learning, as proposed by Bengio et al. <a href="#ref_3">[3]</a>, formalizes this intuition. It suggests that models learn more effectively when samples are presented in a meaningful order of increasing difficulty ("Easy-to-Hard"). Recent adaptive frameworks like EntAugment <a href="#ref_5">[5]</a> and ObjBlur <a href="#ref_4">[4]</a> have successfully applied this principle to data augmentation.
        </p>

        <h3>Anti-Curriculum</h3>
        <p>
            Conversely, a counter-intuitive line of research suggests an "Anti-Curriculum" (Hard-to-Easy) approach. Theoretical work by Maltser <a href="#ref_6">[6]</a> indicates that exposing models to difficult, strongly augmented samples <strong>early on</strong> might force the learning of more robust, shape-invariant features. By front-loading the difficulty, the model may be prevented from learning "shortcuts" (like superficial texture bias) during its initial phase of high plasticity.
        </p>

        <h3>Our Contribution</h3>
        <p>
            In this work, we systematically investigate the <strong>trajectory of difficulty</strong> in data augmentation. We do not merely ask <em>if</em> curriculum learning helps; we ask <em>which trajectory</em> is optimal. We define our core question: <strong>Should augmentation difficulty increase or decrease over training, and does the direction of this curriculum significantly affect generalization on CIFAR-10/100?</strong>
        </p>
        <p>
            We briefly preview our main findings:
        </p>
        <ul>
            <li><strong>Performance Parity:</strong> Contrary to the intuitive expectation, no curriculum direction consistently outperformed the others in terms of best-case test accuracy. All curricula (increasing, decreasing, static) reach similar best-case accuracy.</li>
            <li><strong>Stability over Performance:</strong> The dominant effect of strong augmentation curricula in this regime is on training stability (the frequency of failed runs), not on the quality of the final solution.</li>
            <li><strong>Magnitude of Effect:</strong> Any advantage of "Easy-to-Hard" is small relative to the base hyperparameters and the inherent variance of SGD.</li>
        </ul>
        <p>
            We explore two distinct axes of difficulty:
        </p>
        <ol>
            <li><strong>Complexity-based:</strong> Using <strong>Colorful Cutout</strong> <a href="#ref_7">[7]</a> to vary the information entropy of the occluded region.</li>
            <li><strong>Occlusion-based:</strong> Using <strong>Dynamic Cutout Size</strong>, a method we implemented to vary the spatial extent of information loss.</li>
        </ol>
        <p>
            We compare three distinct training regimes:
        </p>
        <ul>
            <li><strong>Static Baseline:</strong> Random augmentation magnitude (Standard practice).</li>
            <li><strong>Curriculum (Easy &rarr; Hard):</strong> Gradually increasing complexity/occlusion.</li>
            <li><strong>Anti-Curriculum (Hard &rarr; Easy):</strong> Gradually decreasing complexity/occlusion.</li>
        </ul>
        <p>
            By training ResNet50 architectures on CIFAR-10 <a href="#ref_8">[8]</a> and CIFAR-100 under these varying dynamics, we aim to decouple the impact of augmentation <em>type</em> from augmentation <em>scheduling</em>.
        </p>
        <img src="./images/your_intro_diagram.png" width=512px/>
    </div>
    <div class="margin-right-block">
        <div style="margin-bottom: 20px;">
            <strong>Hypothesis:</strong><br>
            Does early exposure to "hard" augmentations (Anti-Curriculum) induce better robustness than the traditional "Easy-to-Hard" Curriculum?
        </div>
    </div>
</div>

<div class="content-margin-container" id="related_work">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
        <h1>2. Related Work</h1>
        <p>
            The field of deep learning has long relied on Data Augmentation (DA) to improve generalization. Classic augmentations include geometric transformations (flips, crops) and occlusion methods like Cutout <a href="#ref_2">[2]</a> and Random Erasing. A key discussion in this field concerns the "strength" of augmentation—stronger augmentations provide better regularization but risk destroying label information.
        </p>
        <p>
            Recent research has pivoted toward <strong>adaptive</strong> strategies that integrate curriculum learning: models learn faster and better when training examples are ordered from easy to hard <a href="#ref_3">[3]</a>.
        </p>
        <ul>
            <li><strong>ObjBlur <a href="#ref_4">[4]</a>:</strong> Introduces a curriculum for layout-to-image generation by progressively moving from strong blurring (low-frequency signals) to cleaner images (high-frequency details).</li>
            <li><strong>EntAugment <a href="#ref_5">[5]</a>:</strong> Proposes a tuning-free framework that adapts augmentation magnitude based on sample difficulty. It utilizes an entropy-based metric to assess model confidence, applying lighter augmentations to hard samples early in training and intensifying the magnitude as the model generalizes.</li>
        </ul>

        <h3>Magnitude-Based Scheduling</h3>
        <p>
            A key subset of Curriculum Data Augmentation focuses specifically on scheduling the <strong>intensity</strong> or <strong>magnitude</strong> of transformations. Wei et al. (2020) demonstrated that gradually increasing the magnitude of distortions (e.g., rotation angle, shear intensity) correlates with improved generalization <a href="#ref_9">[9]</a>.
        </p>
        <p>
            In the context of occlusion methods, this magnitude corresponds directly to the <strong>size</strong> of the erased region. While early occlusion methods like Cutout <a href="#ref_2">[2]</a> and Random Erasing utilized fixed or randomly sampled sizes, later works such as Hide-and-Seek highlighted the delicate balance required between occlusion and feature retention. Our <strong>Dynamic Cutout Size</strong> implementation serves as a direct instantiation of magnitude-based curriculum for occlusion: we explicitly schedule the bounding box dimension ($W \times H$) as a function of the training epoch.
        </p>

        <h3>The Directionality Debate</h3>
        <p>
            Finally, a critical, yet under-explored question is the <strong>trajectory</strong> of augmentation difficulty. While standard CL dictates an "Easy-to-Hard" schedule, recent work by Maltser on <strong>Augmentation Curriculum Learning (ACL)</strong> investigates alternative pacing, including "Anti-Curriculum" (Hard-to-Easy) strategies <a href="#ref_6">[6]</a>. Maltser's findings suggest that explicitly scoring and ordering augmentations can outperform random selection, but the optimal direction remains a subject of debate.
        </p>
        <p>
            In this project, we utilize <strong>Colorful Cutout</strong> <a href="#ref_7">[7]</a> to test these conflicting hypotheses. By progressively dividing the erasure box into smaller sub-regions, Colorful Cutout allows us to control the "complexity" of the noise. As the number of sub-regions increases, the sample becomes more "tangled" and difficult, naturally forming a difficulty gradient that we can invert or scale.
        </p>
    </div>
    <div class="margin-right-block">
        <strong>Key Concept:</strong><br>
        <em>Magnitude Scheduling</em>: Varying the intensity (size) of an augmentation over time.
    </div>
</div>

<div class="content-margin-container" id="methodology">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
        <h1>3. Methodology</h1>
        
        <h3>3.1 Task, Datasets, and Model</h3>
        <p>
            To isolate the effect of the curriculum, we needed a rigorously controlled environment. We didn't want model capacity or hyperparameter tuning to affect the result, so we kept the training recipe the same across augmentations.
        </p>
        <ul style="margin-left: 20px; line-height: 1.6em;">
            <li style="margin-bottom: 10px;"><strong>Task:</strong> Supervised image classification.</li>
            <li style="margin-bottom: 10px;"><strong>Datasets:</strong> We evaluated performance on CIFAR-10 and CIFAR-100. CIFAR images (32×32) are resized to 256×256 then cropped to 244×244 and normalized for ResNet-50. We tested two distinct data availability regimes:
                <ul style="margin-top: 5px; margin-bottom: 5px; list-style-type: circle;">
                    <li><strong>Full Data:</strong> We use the entire training split after the train/validation partition.</li>
                    <li><strong>Low-Data (10%):</strong> We subsample the training split to retain only a fixed fraction <i>f=0.1</i>.</li>
                </ul>
            </li>
            <li><strong>Model:</strong> ResNet-50.</li>
        </ul>
        
        <p>
            Given an input image <i style="font-family: serif;">x</i> &in; &#8477;<sup>3 &times; H &times; W</sup>, the backbone produces a feature tensor which we globally pool and flatten into a vector <i style="font-family: serif;">h</i> &in; &#8477;<sup>d</sup>. The final prediction layer is a small MLP classifier. We train the model to minimize cross-entropy loss with label smoothing to mitigate overconfidence.
        </p>

        <h3>Augmentations</h3>
        <p>
            We investigated two types of augmentation. Each has a difficulty level we can change during training.
        </p>

        <ul style="list-style-type: disc; margin-left: 20px;">
            
            <li style="margin-bottom: 25px;">
                <strong>A. Colorful Cutout (Complexity-based Difficulty)</strong>
                <div style="margin-top: 5px; margin-bottom: 10px;">
                    This augmentation is implemented based on the Colorful Cutout; this places a single box of random noise over the image.
                </div>
                <ul style="list-style-type: circle; margin-left: 20px;">
                    <li style="margin-bottom: 8px;">
                        <strong>Mechanism:</strong> For a bounding box of size <i style="font-family: serif;">B</i>, we define a <code>region_size</code> <i style="font-family: serif;">S</i>. The box is divided into (<i style="font-family: serif;">B/S</i>)<sup>2</sup> sub-squares. Each sub-square is filled with a random color.
                    </li>
                    <li style="margin-bottom: 8px;">
                        <strong>Difficulty Metric:</strong> As <i style="font-family: serif;">S</i> decreases, the number of sub-regions increases, creating higher-frequency color noise (higher entropy).
                    </li>
                    <li style="margin-bottom: 8px;">
                        <strong>Code Implementation:</strong>
                        <div style="margin: 10px 0; text-align: left; padding-left: 20px;">
                             <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline">
                              <mi>S</mi>
                              <mo>=</mo>
                              <mfrac>
                                <mi>B</mi>
                                <msup>
                                  <mn>2</mn>
                                  <mtext>exponent</mtext>
                                </msup>
                              </mfrac>
                            </math>
                        </div>
                        Where <code>exponent</code> is the curriculum variable controlled by the epoch index.
                    </li>
                </ul>
            </li>

            <li style="margin-bottom: 25px;">
                <strong>B. Dynamic Cutout Size (Occlusion-based Difficulty)</strong>
                <div style="margin-top: 5px; margin-bottom: 10px;">
                    This method varies the spatial extent of the occlusion. We dynamically calculate the side length of the square mask, <i style="font-family: serif;">L<sub>mask</sub></i>, based on the current epoch.
                </div>
                <ul style="list-style-type: circle; margin-left: 20px;">
                    <li style="margin-bottom: 8px;">
                        <strong>Mechanism:</strong> We linearly interpolate the mask size between a minimum (<i style="font-family: serif;">L<sub>min</sub></i>) and maximum (<i style="font-family: serif;">L<sub>max</sub></i>) value.
                    </li>
                    <li style="margin-bottom: 8px;">
                        <strong>Difficulty Metric:</strong> Larger <i style="font-family: serif;">L<sub>mask</sub></i> corresponds to greater information loss (higher difficulty).
                    </li>
                    <li style="margin-bottom: 8px;">
                        We implemented a variant of this augmentation with a larger increase in side length in each step.
                    </li>
                </ul>
            </li>
        </ul>

        <h3>Pacing Functions</h3>
        <p>
            We implemented dynamic schedulers that adjust the augmentation parameters as a function of the current epoch <i style="font-family: serif;">t</i> and total epochs <i style="font-family: serif;">T<sub>total</sub></i>:
        </p>
        
        <h4>Increasing Difficulty (Curriculum)</h4>
        <ul style="margin-left: 20px; margin-bottom: 20px;">
            <li style="margin-bottom: 10px;">
                <strong>Colorful Cutout:</strong> We increase the exponent linearly from 0 to <code>max_diff</code>. This decreases the <code>region_size</code>, transitioning from a single solid color block (Easy) to a highly fragmented noise grid (Hard).
            </li>
            <li>
                <strong>Dynamic Size:</strong> The mask size <i style="font-family: serif;">L<sub>mask</sub></i> scales linearly from <i style="font-family: serif;">L<sub>min</sub></i> to <i style="font-family: serif;">L<sub>max</sub></i>.
            </li>
        </ul>

        <h4>Decreasing Difficulty (Anti-Curriculum)</h4>
        <ul style="margin-left: 20px;">
            <li style="margin-bottom: 10px;">
                <strong>Colorful Cutout:</strong> We start with the maximum exponent (high-frequency noise) and decrease to 0 (solid block).
            </li>
            <li>
                <strong>Dynamic Size:</strong> The mask size starts at <i style="font-family: serif;">L<sub>max</sub></i> and shrinks to <i style="font-family: serif;">L<sub>min</sub></i>, revealing more of the image as training progresses.
            </li>
        </ul>
    </div>
    <div class="margin-right-block"></div>
</div>

<div class="content-margin-container" id="experiments">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
        <style>
            /* LaTeX-style Table CSS */
            table.academic-table {
                width: 100%;
                border-collapse: collapse;
                margin: 20px 0;
                font-family: "Georgia", serif;
                font-size: 15px;
                color: #333;
            }
            table.academic-table th {
                border-top: 2px solid #333;
                border-bottom: 1px solid #333;
                padding: 10px;
                text-align: center;
                font-weight: 600;
                background-color: #fff; /* Ensure transparent/white background */
            }
            table.academic-table td {
                padding: 8px;
                text-align: center;
                border: none;
            }
            /* Add a bottom border to the last row of the table */
            table.academic-table tr:last-child td {
                border-bottom: 2px solid #333;
            }
            /* Subtle separator between augmentation groups */
            tr.group-divider td {
                border-bottom: 1px solid #e0e0e0;
            }
            /* Left-align the first column for readability */
            table.academic-table td:first-child, table.academic-table th:first-child {
                text-align: left;
                padding-left: 15px;
            }
        </style>

        <h1>4. Results</h1>
        
        <h3>4.1 Overall Performance by Dataset and Augmentation Family</h3>
        <p>
            The following tables summarize the mean test accuracy and the range (min–max) across multiple random seeds.
        </p>

        <h4>Table 1: CIFAR-10 Performance</h4>
        <table class="academic-table">
            <thead>
                <tr>
                    <th>Augmentation Family</th>
                    <th>Schedule</th>
                    <th>Runs (n)</th>
                    <th>Mean Acc.</th>
                    <th>Std Dev</th>
                    <th>Min</th>
                    <th>Max</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td rowspan="3" style="vertical-align: middle;"><strong>COLOR_CUTOUT</strong></td>
                    <td>no_cur</td>
                    <td>3</td>
                    <td>0.75</td>
                    <td>0.32</td>
                    <td>0.38</td>
                    <td>0.96</td>
                </tr>
                <tr>
                    <td>incr</td>
                    <td>2</td>
                    <td>0.61</td>
                    <td>0.42</td>
                    <td>0.31</td>
                    <td>0.90</td>
                </tr>
                <tr class="group-divider">
                    <td>decr</td>
                    <td>3</td>
                    <td>0.72</td>
                    <td>0.37</td>
                    <td>0.30</td>
                    <td>0.95</td>
                </tr>
                
                <tr>
                    <td rowspan="2" style="vertical-align: middle;"><strong>CUTOUT_DYNAMIC</strong></td>
                    <td>incr</td>
                    <td>3</td>
                    <td>0.74</td>
                    <td>0.34</td>
                    <td>0.35</td>
                    <td>0.96</td>
                </tr>
                <tr class="group-divider">
                    <td>decr</td>
                    <td>3</td>
                    <td>0.71</td>
                    <td>0.38</td>
                    <td>0.28</td>
                    <td>0.95</td>
                </tr>

                <tr>
                    <td rowspan="2" style="vertical-align: middle;"><strong>DYNAMIC_CONTRAST</strong></td>
                    <td>incr</td>
                    <td>3</td>
                    <td>0.72</td>
                    <td>0.36</td>
                    <td>0.30</td>
                    <td>0.95</td>
                </tr>
                <tr>
                    <td>decr</td>
                    <td>3</td>
                    <td>0.71</td>
                    <td>0.37</td>
                    <td>0.28</td>
                    <td>0.95</td>
                </tr>
            </tbody>
        </table>

        <p><strong>Key Observations for CIFAR-10:</strong></p>
        <ul>
            <li><strong>Performance Parity:</strong> Best-case performance is essentially identical across all configurations (Max $\approx 0.96$). Conditional on the run converging successfully, curriculum direction barely matters.</li>
            <li><strong>Impact of Failed Runs:</strong> The low mean scores are driven by specific seeds failing to converge (Min $\approx 0.30$). If we filter for "successful" runs (accuracy $> 0.8$), all configurations cluster tightly in a narrow band of $0.93 \pm 0.02$.</li>
        </ul>

        <h4>Table 2: CIFAR-100 Performance</h4>
        <table class="academic-table">
            <thead>
                <tr>
                    <th>Augmentation Family</th>
                    <th>Schedule</th>
                    <th>Runs (n)</th>
                    <th>Mean Acc.</th>
                    <th>Std Dev</th>
                    <th>Min</th>
                    <th>Max</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td rowspan="3" style="vertical-align: middle;"><strong>COLOR_CUTOUT</strong></td>
                    <td>no_cur</td>
                    <td>2</td>
                    <td>0.73</td>
                    <td>0.12</td>
                    <td>0.64</td>
                    <td>0.81</td>
                </tr>
                <tr>
                    <td>incr</td>
                    <td>8</td>
                    <td>0.66</td>
                    <td>0.06</td>
                    <td>0.64</td>
                    <td>0.81</td>
                </tr>
                <tr class="group-divider">
                    <td>decr</td>
                    <td>2</td>
                    <td>0.73</td>
                    <td>0.11</td>
                    <td>0.65</td>
                    <td>0.80</td>
                </tr>

                <tr>
                    <td rowspan="2" style="vertical-align: middle;"><strong>CUTOUT_DYNAMIC</strong></td>
                    <td>incr</td>
                    <td>2</td>
                    <td>0.71</td>
                    <td>0.14</td>
                    <td>0.61</td>
                    <td>0.81</td>
                </tr>
                <tr class="group-divider">
                    <td>decr</td>
                    <td>2</td>
                    <td>0.71</td>
                    <td>0.14</td>
                    <td>0.62</td>
                    <td>0.81</td>
                </tr>

                <tr>
                    <td rowspan="2" style="vertical-align: middle;"><strong>DYNAMIC_CONTRAST</strong></td>
                    <td>incr</td>
                    <td>2</td>
                    <td>0.68</td>
                    <td>0.16</td>
                    <td>0.56</td>
                    <td>0.79</td>
                </tr>
                <tr>
                    <td>decr</td>
                    <td>2</td>
                    <td>0.69</td>
                    <td>0.17</td>
                    <td>0.57</td>
                    <td>0.80</td>
                </tr>
            </tbody>
        </table>

        <p><strong>Key Observations for CIFAR-100:</strong></p>
        <ul>
            <li><strong>Clustered Peak Performance:</strong> The best runs across all configurations consistently reach $\approx 0.79–0.81$. Once the model converges, neither curriculum direction nor augmentation family dominates the final accuracy.</li>
            <li><strong>Degradation in Means:</strong> Lower mean accuracy in aggressive settings (e.g., Dynamic Contrast) reflects optimization instability rather than a lower performance ceiling. Strong augmentations plus our specific optimizer schedule occasionally pushed the model into worse minima.</li>
        </ul>

        <h3>4.2 Effect of Curriculum Direction</h3>
        <p>
            Differences in mean accuracy between curricula are largely driven by occasional "bad runs" rather than systematic shifts in the typical "good" performance.
        </p>
        <ul>
            <li>On CIFAR-10, all "successful" runs fall within $\approx 0.93 \pm 0.02$.</li>
            <li>On CIFAR-100, all "successful" runs cluster around $\approx 0.80–0.81$.</li>
        </ul>
        <p>
            This supports the statement that augmentation curriculum does not materially change the quality of the good minima the model can reach.
        </p>

        <h3>4.3 Training Stability</h3>
        <p>
            This is where the most interesting story emerges. We define a "bad run" as test accuracy below 0.5 on CIFAR-10 or below 0.7 on CIFAR-100.
        </p>
        <ul>
            <li>Some configs (e.g., strong dynamic + contrast on CIFAR-100) show more fragile behavior: one seed around $0.56$ and one around $0.80$.</li>
            <li>On CIFAR-10, most configurations have the same pattern: two seeds $\approx 0.90+$ and one $\approx 0.30$.</li>
        </ul>
        <p>
            <strong>Interpretation:</strong> Strong augmentations occasionally lead to optimization failures. However, this effect is not clearly aligned with incr vs decr. Both can have bad seeds; the difference is subtle and not obviously significant given our sample sizes.
        </p>

        <h3>4.4 Comparing Augmentation Families</h3>
        <ul>
            <li><strong>CIFAR-10:</strong> Best-case performance is essentially tied ($\approx 0.95$).</li>
            <li><strong>CIFAR-100:</strong> Dynamic and Color Cutout have similar maxima ($\approx 0.81$). Dynamic Contrast is slightly lower on average due to stability issues.</li>
        </ul>
        <p>
            <strong>Conclusion:</strong> The stronger, more structured augmentations (CUTOUT_DYNAMIC_CONTRAST) do not give a clear accuracy gain in this setup. They may increase instability without a compensating boost in peak performance.
        </p>
    </div>
    <div class="margin-right-block">
        Visualizations of test accuracy over epochs could further illustrate the stability differences.
    </div>
</div>

<div class="content-margin-container" id="discussion">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
        <h1>5. Discussion</h1>
        
        <h3>5.1 Why isn’t Easy&rarr;Hard clearly better?</h3>
        <p>
            Theoretically, we might expect Easy&rarr;Hard to help early representation learning and then act as a regularizer, while Hard&rarr;Easy might hurt early optimization. However, our experiments suggest that once a good representation is found, the network is robust enough that the precise path through augmentation difficulty doesn’t matter—at least for ResNet-50 on CIFAR.
        </p>
        <p>Possible explanations include:</p>
        <ul>
            <li><strong>Dataset Complexity:</strong> CIFAR is relatively small; all curricula are strong enough to regularize but not strong enough to fundamentally distort the learning dynamics.</li>
            <li><strong>Difficulty Range:</strong> Our difficulty range may not be extreme enough to create a clear separation between "too easy" and "too hard."</li>
            <li><strong>Optimization Noise:</strong> The optimization noise (SGD stochasticity, initialization) dominates the small second-order effect of curriculum choice.</li>
        </ul>

        <h3>5.2 Interaction between Augmentation and Optimization</h3>
        <p>
            When augmentations are very strong (dynamic + contrast), some runs clearly underperform. This suggests a tri-way interaction between <strong>Augmentation Difficulty</strong>, <strong>Curriculum Schedule</strong>, and <strong>Optimizer Hyperparameters</strong> (LR schedule, batch size).
        </p>
        <p>
            We hypothesize that with better tuning (e.g., slightly lower LR when difficulty is high, or longer warmup), a more consistent benefit of Easy&rarr;Hard might emerge. Conversely, with more extreme augmentations, curriculum might matter more: Hard&rarr;Easy early epochs with heavy corruption could cause irrecoverable damage.
        </p>
        <p>
            Under conventional ResNet-50 training settings on CIFAR-10/100, the choice of augmentation curriculum direction has at most a very small effect on final test accuracy. The primary observable effect is on the distribution of outcomes (<strong>stability</strong>), not on best-case performance. Thus, for practitioners in this regime, spending effort on precise curriculum design is likely less important than robust hyperparameter tuning and choosing a reasonable base augmentation strength.
        </p>

        <h3>5.3 Limitations</h3>
        <ul>
            <li><strong>Sample Size:</strong> We used a small number of seeds per configuration due to computational constraints.</li>
            <li><strong>Scope:</strong> We limited our study to one architecture (ResNet-50) and one family of augmentations (cutout-style).</li>
            <li><strong>Hyperparameters:</strong> We did not tune hyperparameters per curriculum; they all shared the same optimizer and LR schedule.</li>
            <li><strong>Metric:</strong> We analyzed final test accuracy only, without analyzing full training curves or early-epoch behavior.</li>
        </ul>

        <h3>5.4 Future Work</h3>
        <ul>
            <li><strong>Adaptive Curriculum:</strong> Explore curricula where difficulty is adjusted based on training loss or validation accuracy rather than a fixed linear schedule.</li>
            <li><strong>Dimensions:</strong> Explore other augmentation dimensions: color jitter strength, blur, geometric distortions, or RandAugment magnitude schedules.</li>
            <li><strong>Datasets:</strong> Consider more challenging datasets or architectures, where overfitting and representation learning challenges are more severe.</li>
        </ul>

        <h3>5.5 Conclusion</h3>
        <p>
            We evaluated several curricula (increasing, decreasing, static augmentation difficulty) across different cutout-style augmentations on CIFAR-10 and CIFAR-100 with ResNet-50.
        </p>
        <p>
            Contrary to the intuitive expectation, no curriculum direction consistently outperformed the others in terms of best-case test accuracy. The main effect of strong augmentation and curricula in this regime is on <strong>training stability</strong>, not on the quality of the final solution. This suggests that, at least in this standard vision benchmark setting, augmentation curricula are a relatively second-order design choice compared to base augmentation strength.
        </p>
    </div>
    <div class="margin-right-block">
    </div>
</div>

<div class="content-margin-container" id="citations">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
        <div class='citation' id="references" style="height:auto"><br>
            <span style="font-size:16px">References:</span><br><br>

            <a id="ref_1"></a>[1] He, K., et al. "Deep residual learning for image recognition." CVPR 2016.<br><br>
            
            <a id="ref_2"></a>[2] DeVries, T., & Taylor, G. W. "Improved regularization of convolutional neural networks with cutout." arXiv 2017.<br><br>
            
            <a id="ref_3"></a>[3] Bengio, Y., et al. "Curriculum learning." ICML 2009.<br><br>

            <a id="ref_4"></a>[4] Frolov, S., et al. "ObjBlur: A Curriculum Learning Approach With Progressive Object-Level Blurring." arXiv 2024.<br><br>

            <a id="ref_5"></a>[5] Yang, S., et al. "EntAugment: Entropy-Driven Adaptive Data Augmentation Framework." arXiv 2024.<br><br>

            <a id="ref_6"></a>[6] Maltser, R. "Augmentation Curriculum Learning." Master's Thesis, Hebrew University of Jerusalem, 2025.<br><br>

            <a id="ref_7"></a>[7] Choi, J., & Kim, Y. "Colorful Cutout: Enhancing Image Data Augmentation with Curriculum Learning." ICLR Tiny Papers 2024.<br><br>
            
            <a id="ref_8"></a>[8] Krizhevsky, A., et al. "Learning multiple layers of features from tiny images." 2009.<br><br>

            <a id="ref_9"></a>[9] Wei, J., et al. "Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning." NAACL 2021.<br><br>

        </div>
    </div>
    <div class="margin-right-block"></div>
</div>

</body>
</html>