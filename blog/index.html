<!DOCTYPE html>
<html>
<head>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
    /* 1. BASIC PAGE SETUP */
    body {
        background-color: #ffffff; /* Clean white background */
        color: #333333; /* Dark gray text is easier on eyes than pitch black */
        font-family: "Georgia", "Times New Roman", serif; /* Serif font for the main reading */
        font-size: 20px; /* Distill uses large, readable text */
        line-height: 1.6em; /* More breathing room between lines */
        margin: 0;
    }

    /* 2. LINKS (Blue) */
    a:link, a:visited {
        color: #2e6db2; 
        text-decoration: none;
        border-bottom: 1px solid #e0e0e0; /* Subtle underline */
    }
    a:hover {
        color: #1a4e85;
        border-bottom: 2px solid #1a4e85;
    }

    /* 3. LAYOUT GRID */
    .content-margin-container {
        display: flex;
        width: 100%;
        justify-content: center;
        align-items: flex-start; /* Aligns content to top */
        margin-bottom: 20px;
    }

    /* 4. MAIN COLUMN (Narrower for readability) */
    .main-content-block {
        width: 55%; 
        max-width: 720px; /* Optimal reading width */
        background-color: #fff;
        padding: 0px;
        /* We REMOVED the borders here */
        border: none; 
    }

    /* 5. MARGIN NOTES */
    .margin-left-block {
        width: 20%;
        max-width: 180px;
        margin-right: 30px;
        text-align: right;
        font-size: 14px;
        color: #6b6b6b;
        font-family: "HelveticaNeue-Light", "Helvetica Neue", sans-serif; /* Sidenotes stay sans-serif */
        line-height: 1.4em;
    }

    .margin-right-block {
        width: 20%;
        max-width: 220px; /* Slightly wider right margin */
        margin-left: 30px;
        text-align: left;
        font-size: 13px;
        color: #6b6b6b;
        font-family: "HelveticaNeue-Light", "Helvetica Neue", sans-serif;
        line-height: 1.4em;
    }

    /* 6. HEADERS */
    h1, h2, h3 {
        font-family: "HelveticaNeue-Light", "Helvetica Neue", Helvetica, Arial, sans-serif;
        color: #000;
        margin-top: 50px;
        margin-bottom: 15px;
        font-weight: 600;
        line-height: 1.2em;
    }
    h1 { font-size: 36px; letter-spacing: -0.5px; } /* Big section headers */
    h2 { font-size: 26px; border-bottom: 1px solid #eee; padding-bottom: 10px; }
    h3 { font-size: 20px; font-weight: 600; margin-top: 30px;}

    /* 7. IMAGES & CAPTIONS */
    img, .my-video {
        max-width: 100%;
        height: auto;
        display: block;
        margin: 20px auto;
        border-radius: 4px; 
        box-shadow: 0 4px 10px rgba(0,0,0,0.05); /* Subtle shadow for images */
    }

    /* 8. MATH & CODE */
    .mathjax-mobile, .mathml-non-mobile { display: none; }
    .show-mathml .mathml-non-mobile { display: block; }
    .show-mathjax .mathjax-mobile { display: block; }
    
    code {
        background-color: #f4f4f4;
        padding: 2px 5px;
        border-radius: 3px;
        font-family: "Courier New", Courier, monospace;
        font-size: 0.9em;
    }

    /* 9. CITATION BLOCK */
    div.citation {
        font-family: "HelveticaNeue-Light", "Helvetica Neue", sans-serif;
        font-size: 14px;
        background-color: #f7f7f7;
        padding: 20px;
        border-radius: 5px;
        border: 1px solid #eee;
    }

    /* 10. TITLE HEADER TABLE SPECIFIC */
    table.header {
        width: 100%;
        max-width: 100%;
    }

    /* 11. TOP NAVIGATION BAR */
    .nav-header {
        background-color: #23374d; /* Navy Blue */
        width: 100%;
        display: flex;
        justify-content: center;
        padding: 15px 0;
        font-family: "HelveticaNeue-Light", "Helvetica Neue", Helvetica, Arial, sans-serif;
        border-bottom: 1px solid rgba(0,0,0,0.1);
    }
    
    .nav-inner {
        width: 90%; 
        max-width: 1080px; /* Matches the width of the article content */
        display: flex;
        justify-content: space-between;
        align-items: center;
    }

    .nav-logo {
        font-weight: 600;
        font-size: 18px;
        color: white !important; /* Force white text */
        border-bottom: none !important; /* Remove underline */
        display: flex;
        align-items: center;
    }

    .nav-links a {
        color: rgba(255,255,255,0.7) !important; /* Slightly transparent white */
        margin-left: 25px;
        font-size: 12px;
        font-weight: 500;
        text-transform: uppercase; /* MAKES TEXT CAPS */
        letter-spacing: 0.5px;
        border-bottom: none !important;
        transition: color 0.2s;
    }

    .nav-links a:hover {
        color: #ffffff !important; /* Bright white on hover */
    }

</style>

<title>Trajectory of Augmentation Difficulty</title>
<meta property="og:title" content="Trajectory of Augmentation Difficulty" />
<meta charset="UTF-8">
</head>

<body>

<div class="nav-header">
    <div class="nav-inner">
        <a href="#" class="nav-logo">
            <span class="logo-icon"></span> 6.7960 Project
        </a>
        
        <div class="nav-links">
            <a href="#">About</a>
            <a href="#">Paper</a>
            <a href="#">Code</a>
            <a href="#">Team</a>
        </div>
    </div>
</div>

<div class="content-margin-container" style="margin-top: 60px; margin-bottom: 60px;">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
        <h1 style="font-size: 50px; line-height: 1.1em; margin-bottom: 20px; font-family: 'Helvetica Neue', Helvetica, sans-serif;">
            Investigating the Trajectory<br>of Augmentation Difficulty
        </h1>
        
        <div style="font-family: 'Helvetica Neue', sans-serif; font-size: 18px; color: #555;">
            <p>
                <a href="#">Alice Xiao</a>, 
                <a href="#">Aimee Yu</a>, 
                <a href="#">Celinda Zhu</a>
            </p>
            <p style="font-size: 15px; color: #888;">
                Final Project for 6.7960, MIT &bull; Fall 2025
            </p>
        </div>
    </div>
    <div class="margin-right-block"></div>
</div>

<div class="content-margin-container" id="intro">
    <div class="margin-left-block">
        <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
            <b style="font-size:16px">Outline</b><br><br>
            <a href="#intro">Introduction</a><br><br>
            <a href="#related_work">Literature Review</a><br><br>
            <a href="#methodology">Methodology</a><br><br>
            <a href="#experiments">Experiments & Results</a><br><br>
            <a href="#discussion">Discussion</a><br><br>
        </div>
    </div>
    <div class="main-content-block">
        <h1>Introduction</h1>
        <p>
            The success of deep learning in computer vision is heavily predicated on the ability of over-parameterized models, such as ResNet <a href="#ref_1">[1]</a>, to generalize beyond their training data. Data Augmentation (DA) has emerged as a cornerstone technique to bridge the generalization gap, artificially expanding the training distribution through stochastic transformations like cropping, flipping, and occlusion.
        </p>
        <p>
            However, standard DA pipelines typically treat the training process as <strong>static</strong>. Augmentations are applied with random magnitudes drawn from a fixed distribution, regardless of the model's current learning state[cite: 2040]. This creates a potential misalignment: in early training stages, strong augmentations might introduce excessive noise that hinders convergence, while in later stages, weak augmentations may fail to provide sufficient regularization to prevent overfitting.
        </p>
        
        <h3>The Curriculum vs. Anti-Curriculum Hypothesis</h3>
        <p>
            <strong>Curriculum Learning (CL)</strong>, as proposed by Bengio et al., suggests that models—like humans—learn more effectively when samples are presented in a meaningful order of increasing difficulty ("Easy&to;Hard") <a href="#ref_3">[3]</a>. Recent adaptive frameworks like EntAugment <a href="#ref_5">[5]</a> and ObjBlur <a href="#ref_4">[4]</a> have successfully applied this principle to DA.
        </p>
        <p>
            Conversely, a counter-intuitive line of research suggests an <strong>"Anti-Curriculum"</strong> (Hard&to;Easy) approach. Work by Maltser <a href="#ref_6">[6]</a> indicates that exposing models to difficult, strongly augmented samples early on might force the learning of more robust, shape-invariant features, effectively preventing the model from relying on superficial textures or "shortcuts" during the initial plasticity phase[cite: 1347].
        </p>

        <h3>Our Contribution</h3>
        <p>
            In this work, we systematically investigate the <strong>directionality of difficulty</strong> in data augmentation. We utilize two distinct augmentation mechanisms—<strong>Colorful Cutout</strong> <a href="#ref_7">[7]</a> and <strong>Dynamic Cutout Sizing</strong>—to compare three training regimes:
        </p>
        <ul>
            <li><strong>Static Baseline:</strong> Random augmentation magnitude.</li>
            <li><strong>Curriculum (Easy &rarr; Hard):</strong> Gradually increasing complexity/occlusion.</li>
            <li><strong>Anti-Curriculum (Hard &rarr; Easy):</strong> Gradually decreasing complexity/occlusion.</li>
        </ul>
        <p>
            By training ResNet50 architectures on CIFAR-10 under these varying dynamics, we aim to decouple the impact of augmentation <em>type</em> from augmentation <em>scheduling</em>.
        </p>
        <img src="./images/your_intro_diagram.png" width=512px/>
    </div>
    <div class="margin-right-block">
        <div style="margin-bottom: 20px;">
            <strong>Hypothesis:</strong><br>
            Does early exposure to "hard" augmentations (Anti-Curriculum) induce better robustness than the traditional "Easy-to-Hard" Curriculum?
        </div>
    </div>
</div>

<div class="content-margin-container" id="related_work">
    <div class="margin-left-block">
    </div>
    <div class="main-content-block">
        <h1>Literature Review</h1>
        <p>
            Deep learning models rely heavily on Data Augmentation (DA) to improve generalization. While traditional methods such as Cutout, Mixup, and CutMix apply stochastic regularization <a href="#ref_2">[2]</a>, they typically ignore the learning state of the model, applying noise with random magnitudes regardless of sample difficulty.
        </p>

        <h3>Adaptive Augmentation & Curriculum Learning</h3>
        <p>
            Recent research has shifted toward <em>adaptive</em> strategies that integrate Curriculum Learning (CL). The core premise of CL is that models learn more efficiently when samples are ordered from easy to hard <a href="#ref_3">[3]</a>. 
            For example, <strong>ObjBlur</strong> introduces a curriculum for layout-to-image generation by progressively moving from strong blurring (low-frequency signals) to cleaner images (high-frequency details) <a href="#ref_4">[4]</a>. Similarly, <strong>EntAugment</strong> employs an entropy-based metric to assess model confidence, dynamically scaling augmentation magnitude based on the difficulty of the individual sample <a href="#ref_5">[5]</a>.
        </p>

        <h3>The Directionality of Difficulty</h3>
        <p>
            A critical, yet under-explored question is the <em>trajectory</em> of augmentation difficulty. While standard CL dictates an "Easy-to-Hard" schedule, recent work by Maltser on <strong>Augmentation Curriculum Learning (ACL)</strong> investigates alternative pacing, including "Anti-Curriculum" (Hard-to-Easy) strategies <a href="#ref_6">[6]</a>. Maltser's findings suggest that explicitly scoring and ordering augmentations can outperform random selection, but the optimal direction remains a subject of debate.
        </p>

        <h3>Methodology: Colorful Cutout</h3>
        <p>
            To investigate these hypotheses, we utilize <strong>Colorful Cutout</strong> <a href="#ref_7">[7]</a>, a technique that enhances standard Cutout by filling erased regions with random colors and progressively dividing the erasure box into smaller sub-regions. As the number of sub-regions increases, the complexity of the occlusion rises, naturally forming a difficulty gradient[cite: 2065]. We utilize the official implementation (<a href="https://github.com/c-juhwan/colorful-cutout-aug/tree/master">GitHub</a>) to test whether increasing this difficulty (Curriculum) outperforms decreasing it (Anti-Curriculum) or maintaining a static baseline.
        </p>
    </div>
    <div class="margin-right-block">
        <div style="margin-bottom: 20px;">
            <strong>Key Concept:</strong><br>
            <em>Curriculum Learning</em> suggests training on "easy" data before "hard" data to improve convergence.
        </div>
        <div style="margin-bottom: 20px;">
            <strong>Our Focus:</strong><br>
            Does the direction of difficulty (Easy&rarr;Hard vs. Hard&rarr;Easy) matter for Data Augmentation?
        </div>
        
        <hr style="border-top: 1px solid #eee; margin: 20px 0;">
        
        Cite papers using the reference links at the bottom.
    </div>
</div>

<div class="content-margin-container" id="methodology">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
        <h1>Methodology</h1>
        <p>
            Our experimental framework is built on PyTorch, utilizing the <strong>CIFAR-10</strong> dataset <a href="#ref_8">[8]</a> and a <strong>ResNet50</strong> backbone <a href="#ref_1">[1]</a>. To isolate the effects of curriculum pacing, we maintain consistent hyperparameters across all runs (Batch Size=32, Learning Rate=5e-5), modifying only the augmentation pipeline.
        </p>

        <h3>Defining "Difficulty"</h3>
        <p>
            We define difficulty through the lens of information loss and signal complexity. We explore two specific augmentation modalities:
        </p>
        
        <h4>1. Colorful Cutout (Complexity-based)</h4>
        <p>
            Building on the work of Choi & Kim <a href="#ref_7">[7]</a>, this method occludes a portion of the image with a grid of random colors rather than black pixels.
        </p>
        <ul>
            <li><strong>Easy:</strong> Low number of sub-regions ($N$); resembles a solid color block.</li>
            <li><strong>Hard:</strong> High number of sub-regions ($N$); introduces high-frequency color noise.</li>
        </ul>
        

        <h4>2. Dynamic Cutout Size (Occlusion-based)</h4>
        <p>
            A standard cutout approach where the <em>size</em> of the occluded region varies dynamically.
        </p>
        <ul>
            <li><strong>Easy:</strong> Small mask (minimal information loss).</li>
            <li><strong>Hard:</strong> Large mask (significant semantic occlusion).</li>
        </ul>
        

        <h3>Pacing Functions</h3>
        <p>
            We implemented dynamic schedulers that adjust the augmentation parameters ($N$ for Colorful Cutout, $Size$ for Dynamic Cutout) as a linear function of the training epoch $t$:
        </p>
        <center>
            <p style="font-family: courier;">
                Difficulty(t) &propto; t / T_total &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (Curriculum: Easy &rarr; Hard)<br>
                Difficulty(t) &propto; 1 - (t / T_total) &nbsp; (Anti-Curriculum: Hard &rarr; Easy)
            </p>
        </center>
        <p>
           This allows us to test the trajectory of learning without changing the total amount of augmentation applied over the course of training.
        </p>
    </div>
    <div class="margin-right-block">
        <strong>Implementation:</strong><br>
        We utilize a modular argument parser to switch between <code>cutout_dynamic_cur_incr</code> and <code>cutout_dynamic_cur_decr</code>.
    </div>
</div>

<div class="content-margin-container" id="experiments">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
        <h1>Experiments & Results</h1>
        <p>
            <em>(Results pending final training runs)</em>
        </p>
        
        <p>
            We compare the test accuracy of ResNet50 on CIFAR-10 across three conditions: Static Baseline, Increasing Difficulty, and Decreasing Difficulty.
        </p>
    </div>
    <div class="margin-right-block">
        Visualizations are critical. Ensure captions explain the main takeaway of the figure.
    </div>
</div>

<div class="content-margin-container" id="discussion">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
        <h1>Discussion & Limitations</h1>
        
        <h3>Discussion</h3>
        <p>
            Our investigation aims to reveal whether the "Easy-to-Hard" intuition holds for data augmentation. If the Curriculum strategy dominates, it suggests that deep networks require a "warm-up" period to learn basic features before handling noise. However, if the Anti-Curriculum strategy proves effective, it would imply that early regularization is crucial for preventing the optimization trajectory from falling into sharp, non-generalizable local minima.
        </p>

        <h3>Limitations</h3>
        <p>
            <strong>Dataset Scale:</strong> Our experiments are limited to CIFAR-10. The dynamics of curriculum learning may differ significantly on higher-resolution datasets like ImageNet, where texture bias is more pronounced.
        </p>
        <p>
            <strong>Training Duration:</strong> Due to computational constraints, our experiments utilized shortened training cycles (5 epochs). While sufficient to observe convergence trends, long-term training dynamics (e.g., late-stage overfitting) might reveal different behaviors in a full production setting.
        </p>
        <p>
            <strong>Augmentation Scope:</strong> We focused exclusively on occlusion-based augmentations (Cutout variants). Geometric augmentations (Rotation, Warping) might respond differently to curriculum pacing.
        </p>
    </div>
    <div class="margin-right-block">
        Future work could explore <strong>oscillating schedules</strong> (Cyclical Learning Rates applied to augmentation).
    </div>
</div>

<div class="content-margin-container" id="citations">
    <div class="margin-left-block"></div>
    <div class="main-content-block">
        <div class='citation' id="references" style="height:auto"><br>
            <span style="font-size:16px">References:</span><br><br>

            <a id="ref_1"></a>[1] He, K., et al. "Deep residual learning for image recognition." CVPR 2016.<br><br>
            
            <a id="ref_2"></a>[2] DeVries, T., & Taylor, G. W. "Improved regularization of convolutional neural networks with cutout." arXiv 2017.<br><br>
            
            <a id="ref_3"></a>[3] Bengio, Y., et al. "Curriculum learning." ICML 2009.<br><br>

            <a id="ref_4"></a>[4] Frolov, S., et al. "ObjBlur: A Curriculum Learning Approach With Progressive Object-Level Blurring." arXiv 2024.<br><br>

            <a id="ref_5"></a>[5] Yang, S., et al. "EntAugment: Entropy-Driven Adaptive Data Augmentation Framework." arXiv 2024.<br><br>

            <a id="ref_6"></a>[6] Maltser, R. "Augmentation Curriculum Learning." Master's Thesis, Hebrew University of Jerusalem, 2025.<br><br>

            <a id="ref_7"></a>[7] Choi, J., & Kim, Y. "Colorful Cutout: Enhancing Image Data Augmentation with Curriculum Learning." ICLR Tiny Papers 2024.<br><br>
            
            <a id="ref_8"></a>[8] Krizhevsky, A., et al. "Learning multiple layers of features from tiny images." 2009.<br><br>

        </div>
    </div>
    <div class="margin-right-block"></div>
</div>

</body>
</html>